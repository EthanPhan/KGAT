{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google colab setup. Uncomment the following cell if the notebook is running in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!git clone https://github.com/EthanPhan/KGAT.git\n",
    "!cp -r KGAT/* .\n",
    "\n",
    "!pip install googledrivedownloader\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1WAkulkadNFiOmH3uu6JH1QC3XkAkASlq',\n",
    "                                    dest_path='./data/FB15k-237/2hop.pickle',\n",
    "                                    unzip=False)\n",
    "\n",
    "!ls ./data/FB15k-237/\n",
    "\n",
    "# Install TensorFlow 2.0\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models import SpKBGATModified\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from preprocess import read_entity_from_id, read_relation_from_id, init_embeddings, build_data\n",
    "from create_batch import Corpus\n",
    "from utils import save_model\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to pass arround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data = \"./data/FB15k-237/\"\n",
    "    epochs_gat = 3000\n",
    "    epochs_conv = 150\n",
    "    weight_decay_gat = 1e-5\n",
    "    weight_decay_conv = 1e-5\n",
    "    pretrained_emb = True\n",
    "    embedding_size = 50\n",
    "    lr = 1e-3\n",
    "    get_2hop = True\n",
    "    use_2hop = True\n",
    "    partial_2hop = True\n",
    "    output_folder = \"./checkpoints/fb/out/\"\n",
    "    batch_size_gat = 272115\n",
    "    valid_invalid_ratio_gat = 2\n",
    "    drop_GAT = 0.3\n",
    "    entity_out_dim = [100, 200]\n",
    "    nheads_GAT = [2, 2]\n",
    "    margin = 1\n",
    "    batch_size_conv = 128\n",
    "    valid_invalid_ratio_conv = 40\n",
    "    out_channels = 50\n",
    "    drop_conv = 0.3\n",
    "    \n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique_entities -> 14505\n",
      "number of unique_entities -> 9809\n",
      "number of unique_entities -> 10348\n",
      "Initialised relations and entities from TransE\n",
      "Graph created\n",
      "Total triples count 310116, training triples 272115, validation_triples 17535, test_triples 20466\n"
     ]
    }
   ],
   "source": [
    "def load_data(args):\n",
    "    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\n",
    "                                                                 os.path.join(args.data, 'relation2vec.txt'))\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        entity_embeddings = np.random.randn(\n",
    "            len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(\n",
    "            len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\n",
    "\n",
    "    return corpus, entity_embeddings, relation_embeddings\n",
    "\n",
    "Corpus_, entity_embeddings, relation_embeddings = load_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings_copied = deepcopy(entity_embeddings)\n",
    "relation_embeddings_copied = deepcopy(relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):\n",
    "    len_pos_triples = int(\n",
    "        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))\n",
    "\n",
    "    pos_triples = train_indices[:len_pos_triples]\n",
    "    neg_triples = train_indices[len_pos_triples:]\n",
    "    pos_triples = np.tile(pos_triples,(int(args.valid_invalid_ratio_gat), 1))\n",
    "\n",
    "    norm_entity = tf.nn.l2_normalize(entity_embed, axis=1)\n",
    "    norm_relation = tf.nn.l2_normalize(relation_embed, axis=1)\n",
    "\n",
    "    pos_source_embeds = tf.nn.embedding_lookup(norm_entity, pos_triples[:, 0])\n",
    "    pos_relation_embeds = tf.nn.embedding_lookup(norm_relation, pos_triples[:, 1])\n",
    "    pos_tail_embeds = tf.nn.embedding_lookup(norm_entity, pos_triples[:, 2])\n",
    "\n",
    "    neg_source_embeds = tf.nn.embedding_lookup(norm_entity, neg_triples[:, 0])\n",
    "    neg_relation_embeds = tf.nn.embedding_lookup(norm_relation, neg_triples[:, 1])\n",
    "    neg_tail_embeds = tf.nn.embedding_lookup(norm_entity, neg_triples[:, 2])\n",
    "\n",
    "    score_positive = tf.reduce_sum(tf.abs(pos_source_embeds + \\\n",
    "                                          pos_relation_embeds - \\\n",
    "                                          pos_tail_embeds),\n",
    "                                   axis = 1)\n",
    "    score_negative = tf.reduce_sum(tf.abs(neg_source_embeds + \\\n",
    "                                          neg_relation_embeds - \\\n",
    "                                          neg_tail_embeds),\n",
    "                                   axis = 1)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.maximum(0., score_positive + args.margin - score_negative))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat(args):\n",
    "\n",
    "    # Creating the gat model here.\n",
    "    ####################################\n",
    "\n",
    "    print(\"Defining model\")\n",
    "\n",
    "    print(\n",
    "        \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\n",
    "    gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.nheads_GAT)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        args.lr,\n",
    "        decay_steps=500,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, decay=args.weight_decay_gat)\n",
    "\n",
    "    gat_loss_func = tf.keras.losses.Hinge()\n",
    "\n",
    "    current_batch_2hop_indices = np.array([])\n",
    "    if(args.use_2hop):\n",
    "        current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args)\n",
    "\n",
    "    epoch_losses = []   # losses of all epochs\n",
    "    print(\"Number of epochs {}\".format(args.epochs_gat))\n",
    "\n",
    "    for epoch in range(args.epochs_gat):\n",
    "        print(\"\\nepoch-> \", epoch)\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_gat == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_gat\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_gat) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "                train_indices = np.array(train_indices)\n",
    "                train_values = np.array(train_values)\n",
    "\n",
    "                # forward pass\n",
    "                entity_embed, relation_embed = gat([Corpus_.train_adj_matrix,\n",
    "                                                    train_indices,\n",
    "                                                    current_batch_2hop_indices])\n",
    "\n",
    "                # calculate loss\n",
    "                loss = batch_gat_loss(\n",
    "                    gat_loss_func, train_indices, entity_embed, relation_embed)\n",
    "                \n",
    "            grads = tape.gradient(loss, gat.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, gat.trainable_weights))\n",
    "\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "            print('Iteration ', iters, loss.numpy())\n",
    "\n",
    "        print(\"Epoch {} , average loss {}\".format(\n",
    "            epoch, sum(epoch_loss) / len(epoch_loss)))\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "\n",
      "Model type -> GAT layer with 2 heads used , Initital Embeddings training\n",
      "length of unique_entities  14505\n",
      "Number of epochs 3000\n",
      "\n",
      "epoch->  0\n",
      "WARNING:tensorflow:From /Volumes/Data/ethan/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1430: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Volumes/Data/ethan/.env_tf20_p37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "Iteration  0 0.96977437\n",
      "Epoch 0 , average loss 0.9697743654251099\n",
      "\n",
      "epoch->  1\n"
     ]
    }
   ],
   "source": [
    "train_gat(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
